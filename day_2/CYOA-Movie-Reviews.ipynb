{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Your Own Adventure: Movie Review Classification\n",
    "\n",
    "## Dataset Info\n",
    "\n",
    "This is a classic sentiment analysis dataset from Pang and Lee, 2004. http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "## Model Info\n",
    "\n",
    "Build Your Own Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "os.environ['OMP_NUM_THREADS'] = '4' \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from vocabulary import Vocabulary\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "START_TOKEN = \"^\"\n",
    "END_TOKEN = \"_\"\n",
    "NLTK_PATH = \"../data/nltk\"\n",
    "import nltk\n",
    "if NLTK_PATH not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_PATH)\n",
    "    \n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(x_data_list):\n",
    "    \"\"\"Count the tokens in the data list\n",
    "    \n",
    "    Args:\n",
    "        x_data_list (list(list(str))): a list of lists, each sublist is a list of string tokens. \n",
    "            In other words, a list of the data points where the data points have been tokenized.\n",
    "    Returns:\n",
    "        dict: a mapping from tokens to their counts \n",
    "    \n",
    "    \"\"\"\n",
    "    # alternatively\n",
    "    # return Counter([token for x_data in x_data_list for token in x_data])\n",
    "    counter = Counter()\n",
    "    for x_data in x_data_list:\n",
    "        for token in x_data:\n",
    "            counter[token] += 1\n",
    "    return counter\n",
    "\n",
    "def add_splits(df, target_y_column, split_proportions=(0.7, 0.15, 0.15), seed=0):\n",
    "    \"\"\"Add 'train', 'val', and 'test' splits to the dataset\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the data frame to assign splits to\n",
    "        target_y_column (str): the name of the label column; in order to\n",
    "            preserve the class distribution between splits, the label column\n",
    "            is used to group the datapoints and splits are assigned within these groups.\n",
    "        split_proportions (tuple(float, float, float)): three floats which represent the\n",
    "            proportion in 'train', 'val, 'and 'test'. Must sum to 1. \n",
    "        seed (int): the random seed for making the shuffling deterministic. If the dataset and seed\n",
    "            are kept the same, the split assignment is deterministic. \n",
    "    Returns:\n",
    "        pd.DataFrame: the input dataframe with a new column for split assignments; note: row order\n",
    "            will have changed.\n",
    "            \n",
    "    \"\"\"\n",
    "    df_by_label = {label: [] for label in df[target_y_column].unique()}\n",
    "    for _, row in df.iterrows():\n",
    "        df_by_label[row[target_y_column]].append(row.to_dict())\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    assert sum(split_proportions) == 1, \"`split_proportions` should sum to 1\"\n",
    "    train_p, val_p, test_p = split_proportions\n",
    "    \n",
    "    out_df = []\n",
    "    # to ensure consistent behavior, lexicographically sort the dictionary\n",
    "    for _, data_points in sorted(df_by_label.items()):\n",
    "        np.random.shuffle(data_points)\n",
    "        n_total = len(data_points)\n",
    "        n_train = int(train_p * n_total)\n",
    "        n_val = int(val_p * n_total)\n",
    "        \n",
    "        for data_point in data_points[:n_train]:\n",
    "            data_point['split'] = 'train'\n",
    "            \n",
    "        for data_point in data_points[n_train:n_train+n_val]:\n",
    "            data_point['split'] = 'val'\n",
    "            \n",
    "        for data_point in data_points[n_train+n_val:]:\n",
    "            data_point['split'] = 'test'\n",
    "        \n",
    "        out_df.extend(data_points)\n",
    "    \n",
    "    return pd.DataFrame(out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Text Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedTextVectorizer:\n",
    "    \"\"\"A composite data structure that uses Vocabularies to map text and its labels to integers\n",
    "    \n",
    "    Attributes:\n",
    "        token_vocab (Vocabulary): the vocabulary managing the mapping between text tokens and \n",
    "            the unique indices that represent them\n",
    "        label_voab (Vocabulary): the vocabulary managing the mapping between labels and the\n",
    "            unique indices that represent them.\n",
    "        max_seq_length (int): the length of the longest sequence (including start or end tokens\n",
    "            that will be prepended or appended).\n",
    "    \"\"\"\n",
    "    def __init__(self, token_vocab, label_vocab, max_seq_length):\n",
    "        \"\"\"Initialize the SupervisedTextVectorizer\n",
    "        \n",
    "        Args:\n",
    "            token_vocab (Vocabulary): the vocabulary managing the mapping between text tokens and \n",
    "                the unique indices that represent them\n",
    "            label_voab (Vocabulary): the vocabulary managing the mapping between labels and the\n",
    "                unique indices that represent them.\n",
    "            max_seq_length (int): the length of the longest sequence (including start or end tokens\n",
    "                that will be prepended or appended).\n",
    "        \"\"\"\n",
    "        self.token_vocab = token_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def _wrap_with_start_end(self, x_data):\n",
    "        \"\"\"Prepend the start token and append the end token.\n",
    "        \n",
    "        Args:\n",
    "            x_data (list(str)): the list of string tokens in the data point\n",
    "        Returns:\n",
    "            list(str): the list of string tokens with start token prepended and end token appended\n",
    "        \"\"\"\n",
    "        return [self.token_vocab.start_token] + x_data + [self.token_vocab.end_token]\n",
    "    \n",
    "    def vectorize(self, x_data, y_label):\n",
    "        \"\"\"Convert the data point and its label into their integer form\n",
    "        \n",
    "        Args:\n",
    "            x_data (list(str)): the list of string tokens in the data point\n",
    "            y_label (str,int): the label associated with the data point\n",
    "        Returns:\n",
    "            numpy.ndarray, int: x_data in vector form, padded to the max_seq_length; and \n",
    "                the label mapped to the integer that represents it\n",
    "        \"\"\"\n",
    "        x_data = self._wrap_with_start_end(x_data)\n",
    "        x_vector = np.zeros(self.max_seq_length).astype(np.int64)\n",
    "        x_data_indices = [self.token_vocab[token] for token in x_data]\n",
    "        x_vector[:len(x_data_indices)] = x_data_indices\n",
    "        y_index = self.label_vocab[y_label]\n",
    "        return x_vector, y_index\n",
    "    \n",
    "    def transform(self, x_data_list, y_label_list):\n",
    "        \"\"\"Transform a dataset by vectorizing each datapoint\n",
    "        \n",
    "        Args: \n",
    "            x_data_list (list(list(str))): a list of lists, each sublist contains string tokens\n",
    "            y_label_list (list(str,int)): a list of either strings or integers. the y label can come\n",
    "                as strings or integers, but they are remapped with the label_vocab to a unique integer\n",
    "        Returns:\n",
    "            np.ndarray(matrix), np.ndarray(vector): the vectorized x (matrix) and vectorized y (vector) \n",
    "        \"\"\"\n",
    "        x_matrix = []\n",
    "        y_vector = []\n",
    "        for x_data, y_label in zip(x_data_list, y_label_list):\n",
    "            x_vector, y_index = self.vectorize(x_data, y_label)\n",
    "            x_matrix.append(x_vector)\n",
    "            y_vector.append(y_index)\n",
    "        \n",
    "        return np.stack(x_matrix), np.stack(y_vector)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_df(cls, df, target_x_column, target_y_column, token_count_cutoff=0):\n",
    "        \"\"\"Instantiate the SupervisedTextVectorizer from a standardized dataframe\n",
    "        \n",
    "        Standardized DataFrame has a special meaning:\n",
    "            there is a column that has been tokenized into a list of strings\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): the dataset with a tokenized text column and a label column\n",
    "            target_x_column (str): the name of the tokenized text column\n",
    "            target_y_column (str): the name of the label column\n",
    "            token_count_cutoff (int): [default=0] the minimum token frequency to add to the\n",
    "                token_vocab.  Any tokens that are less frequent will not be added.\n",
    "        Returns:\n",
    "            SupervisedTextVectorizer: the instantiated vectorizer\n",
    "        \"\"\"\n",
    "        # get the x data (the observations)\n",
    "        target_x_list = df[target_x_column].tolist()\n",
    "        # compute max sequence length, add 2 for the start, end tokens\n",
    "        max_seq_length = max(map(len, target_x_list)) + 2 \n",
    "        \n",
    "        # populate token vocab        \n",
    "        token_vocab = Vocabulary(use_unks=False,\n",
    "                                 use_mask=True,\n",
    "                                 use_start_end=True,\n",
    "                                 start_token=START_TOKEN,\n",
    "                                 end_token=END_TOKEN)\n",
    "        counts = count_tokens(target_x_list)\n",
    "        # sort counts in reverse order\n",
    "        for token, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            if count < token_count_cutoff:\n",
    "                break\n",
    "            token_vocab.add(token)\n",
    "\n",
    "        # populate label vocab\n",
    "        label_vocab = Vocabulary(use_unks=False, use_start_end=False, use_mask=False)\n",
    "        # add the sorted unique labels \n",
    "        label_vocab.add_many(sorted(df[target_y_column].unique()))\n",
    "        \n",
    "        return cls(token_vocab, label_vocab, max_seq_length)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the vectorizer using json to the file specified\n",
    "        \n",
    "        Args:\n",
    "            filename (str): the output file\n",
    "        \"\"\"\n",
    "        vec_dict = {\"token_vocab\": self.token_vocab.get_serializable_contents(),\n",
    "                    \"label_vocab\": self.label_vocab.get_serializable_contents(),\n",
    "                    'max_seq_length': self.max_seq_length}\n",
    "\n",
    "        with open(filename, \"wb\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        \"\"\"Load the vectorizer from the json file it was saved to\n",
    "        \n",
    "        Args:\n",
    "            filename (str): the file into which the vectorizer was saved.\n",
    "        Returns:\n",
    "            SupervisedTextVectorizer: the instantiated vectorizer\n",
    "        \"\"\"\n",
    "        with open(filename, \"rb\") as fp:\n",
    "            contents = json.load(fp)\n",
    "\n",
    "        contents[\"token_vocab\"] = Vocabulary.deserialize_from_contents(contents[\"token_vocab\"])\n",
    "        contents[\"label_vocab\"] = Vocabulary.deserialize_from_contents(contents[\"label_vocab\"])\n",
    "        return cls(**contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        vectorizer (SupervisedTextVectorizer): an instantiated vectorizer\n",
    "        active_split (str): the string name of the active split\n",
    "        \n",
    "        # internal use\n",
    "        _split_df (dict): a mapping from split name to partitioned DataFrame\n",
    "        _vectorized (dict): a mapping from split to an x data matrix and y vector\n",
    "        _active_df (pd.DataFrame): the DataFrame corresponding to the split\n",
    "        _active_x (np.ndarray): a matrix of the vectorized text data\n",
    "        _active_y (np.ndarray): a vector of the vectorized labels\n",
    "    \"\"\"\n",
    "    def __init__(self, df, vectorizer, target_x_column, target_y_column):\n",
    "        \"\"\"Initialize the SupervisedTextDataset\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): the dataset with a text and label column\n",
    "            vectorizer (SupervisedTextVectorizer): an instantiated vectorizer\n",
    "            target_x_column (str): the column containing the tokenized text\n",
    "            target_y_column (str): the column containing the label\n",
    "        \"\"\"\n",
    "        self._split_df = {\n",
    "            'train': df[df.split=='train'],\n",
    "            'val': df[df.split=='val'],\n",
    "            'test': df[df.split=='test']\n",
    "        }\n",
    "        \n",
    "        self._vectorized = {}\n",
    "        for split_name, split_df in self._split_df.items():\n",
    "            self._vectorized[split_name] = \\\n",
    "                vectorizer.transform(x_data_list=split_df[target_x_column].tolist(), \n",
    "                                     y_label_list=split_df[target_y_column].tolist())\n",
    "        self.vectorizer = vectorizer\n",
    "        self.active_split = None\n",
    "        self._active_df = None\n",
    "        self._active_x = None\n",
    "        self._active_y = None\n",
    "        \n",
    "        self.set_split(\"train\")\n",
    "        \n",
    "    def set_split(self, split_name):\n",
    "        \"\"\"Set the active split\n",
    "        \n",
    "        Args:\n",
    "            split_name (str): the name of the split to make active; should\n",
    "                be one of 'train', 'val', or 'test'\n",
    "        \"\"\"\n",
    "        self.active_split = split_name\n",
    "        self._active_x, self._active_y = self._vectorized[split_name]\n",
    "        self._active_df = self._split_df[split_name]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return the data point corresponding to the index\n",
    "        \n",
    "        Args:\n",
    "            index (int): an int between 0 and len(self._active_x)\n",
    "        Returns:\n",
    "            dict: the data for this data point. Has the following form:\n",
    "                {\"x_data\": the vectorized text data point, \n",
    "                 \"y_target\": the index of the label for this data point, \n",
    "                 \"x_lengths\": method: the number of nonzeros in the vector,\n",
    "                 \"data_index\": the provided index for bookkeeping}\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"x_data\": self._active_x[index],\n",
    "            \"y_target\": self._active_y[index],\n",
    "            \"x_lengths\": len(self._active_x[index].nonzero()[0]),\n",
    "            \"data_index\": index\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"The length of the active dataset\n",
    "        \n",
    "        Returns:\n",
    "            int: len(self._active_x)\n",
    "        \"\"\"\n",
    "        return self._active_x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_review_dataframe():\n",
    "    data = []\n",
    "    for category in movie_reviews.categories():\n",
    "        for fileid in movie_reviews.fileids(category):\n",
    "            data.append({\n",
    "                \"text\": \" \".join(movie_reviews.words(fileid)),\n",
    "                \"label\": category,\n",
    "                \"fileid\": fileid\n",
    "            })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "['neg' 'pos']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>fileid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg/cv000_29416.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard ' s quick movie review damn ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg/cv001_19502.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg/cv002_17424.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' first...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg/cv003_12683.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg/cv004_12641.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  plot : two teen couples go to a church party ,...   neg   \n",
       "1  the happy bastard ' s quick movie review damn ...   neg   \n",
       "2  it is movies like these that make a jaded movi...   neg   \n",
       "3  \" quest for camelot \" is warner bros . ' first...   neg   \n",
       "4  synopsis : a mentally unstable man undergoing ...   neg   \n",
       "\n",
       "                fileid  \n",
       "0  neg/cv000_29416.txt  \n",
       "1  neg/cv001_19502.txt  \n",
       "2  neg/cv002_17424.txt  \n",
       "3  neg/cv003_12683.txt  \n",
       "4  neg/cv004_12641.txt  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_movie_review_dataframe()\n",
    "print(len(df))\n",
    "print(df.label.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_tokenizer(input_string):\n",
    "    \"\"\"Tokenized a string a list of its characters\n",
    "    \n",
    "    Args:\n",
    "        input_string (str): the character string to tokenize\n",
    "    Returns:\n",
    "        list: a list of characters\n",
    "    \"\"\"\n",
    "    return list(input_string.lower())\n",
    "\n",
    "\n",
    "def simple_word_tokenizer(text):\n",
    "    \"\"\"Tokenize a sentence string into a list of words\n",
    "    \n",
    "    Args:\n",
    "        text (str): the sentence string to tokenize\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text.lower())\n",
    "    text = re.sub(\"[ ][ ]+\", \" \", text)\n",
    "    return [tok for tok in text.split(\" \") if len(tok) > 0]\n",
    "\n",
    "def make_movie_review_dataset(tokenizer_func):\n",
    "    \"\"\"Make the Movie Review dataset\n",
    "    \n",
    "    Args:\n",
    "        tokenizer_func (function): the tokenizing function to turn each datapoint into \n",
    "            its tokenized form\n",
    "    \"\"\"\n",
    "    df = add_splits(get_movie_review_dataframe(), target_y_column='label')\n",
    "    df['tokenized'] = df.text.apply(tokenizer_func)\n",
    "    vectorizer = SupervisedTextVectorizer.from_df(df, \n",
    "                                                  target_x_column='tokenized', \n",
    "                                                  target_y_column='label')\n",
    "    dataset = SupervisedTextDataset(df=df, \n",
    "                                    vectorizer=vectorizer, \n",
    "                                    target_x_column='tokenized', \n",
    "                                    target_y_column='label')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify it loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([    1, 14797,     8, ...,     0,     0,     0]),\n",
       " 'y_target': 0,\n",
       " 'x_lengths': 313,\n",
       " 'data_index': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = make_movie_review_dataset(simple_word_tokenizer)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 50., 362., 525., 290.,  99.,  43.,  16.,  11.,   1.,   3.]),\n",
       " array([  21.,  307.,  593.,  879., 1165., 1451., 1737., 2023., 2309.,\n",
       "        2595., 2881.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([item['x_lengths'] for item in dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Fill this part in :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # model hyper parameters\n",
    "    # ADD OTHER MODEL HYPER PARAMETERS HERE\n",
    "    num_embeddings=-1,\n",
    "    num_classes=-1,\n",
    "    # training options\n",
    "    batch_size = 128,\n",
    "    cuda=False,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    patience_threshold=3,\n",
    ")\n",
    "\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "args.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Utiltiies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    \"\"\"Compute the accuracy between a matrix of predictions and a vector of label indices\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.FloatTensor): [shape=(batch_size, num_classes)]\n",
    "            The matrix of predictions\n",
    "        y_true (torch.FloatTensor): [shape=(batch_size,)]\n",
    "            The vector of label indices\n",
    "    \"\"\"\n",
    "    y_pred_indices = y_pred.argmax(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\", dataloader_kwargs=None): \n",
    "    \"\"\"Generate batches from a dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): the instantiated dataset\n",
    "        batch_size (int): the size of the batches\n",
    "        shuffle (bool): [default=True] batches are formed from shuffled indices\n",
    "        drop_last (bool): [default=True] don't return the final batch if it's smaller\n",
    "            than the specified batch size\n",
    "        device (str): [default=\"cpu\"] the device to move the tensors to\n",
    "        dataloader_kwargs (dict or None): [default=None] Any additional arguments to the\n",
    "            DataLoader can be specified\n",
    "    Yields:\n",
    "        dict: a dictionary mapping from tensor name to tensor object where the first\n",
    "            dimension of tensor object is the batch dimension\n",
    "    Note: \n",
    "        This function is mostly an iterator for the DataLoader, but has the added\n",
    "        feature that it moves the tensors to a target device. \n",
    "    \"\"\"\n",
    "    dataloader_kwargs = dataloader_kwargs or {}\n",
    "    \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last, **dataloader_kwargs)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "\n",
    "class TrainState:\n",
    "    \"\"\"A data structure for managing training state operations.\n",
    "    \n",
    "    The TrainState will monitor validation loss and everytime a new best loss\n",
    "        (lower is better) is observed, a couple things happen:\n",
    "        \n",
    "        1. The model is checkpointed\n",
    "        2. Patience is reset\n",
    "    \n",
    "    Attributes:\n",
    "        model (torch.nn.Module): the model being trained and will be\n",
    "            checkpointed during training.\n",
    "        dataset (SupervisedTextDataset, TextSequenceDataset): the dataset \n",
    "            which is being iterate during training; must have the `active_split`\n",
    "            attribute. \n",
    "        log_dir (str): the directory to output the checkpointed model \n",
    "        patience (int): the number of epochs since a new best loss was observed\n",
    "        \n",
    "        # Internal Use\n",
    "        _full_model_path (str): `log_dir/model_state_file`\n",
    "        _split (str): the active split\n",
    "        _best_loss (float): the best observed loss\n",
    "    \"\"\"\n",
    "    def __init__(self, model, dataset, log_dir, model_state_file=\"model.pth\"):\n",
    "        \"\"\"Initialize the TrainState\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): the model to be checkpointed during training\n",
    "            dataset (SupervisedTextDataset, TextSequenceDataset): the dataset \n",
    "                which is being iterate during training; must have the `active_split`\n",
    "                attribute. \n",
    "            log_dir (str): the directory to output the checkpointed model \n",
    "            model_state_file (str): the name of the checkpoint model\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self._full_model_path = os.path.join(log_dir, model_state_file)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self._metrics_by_split = {\n",
    "            'train': {}, \n",
    "            'val': {}, \n",
    "            'test': {}\n",
    "        }\n",
    "        \n",
    "        self._split = 'train'\n",
    "        self._best_loss = 10**10\n",
    "        self.patience = 0\n",
    "        \n",
    "    def _init_metric(self, split, metric_name):\n",
    "        \"\"\"Initialize a metric to the specified split\n",
    "        \n",
    "        A dictionary is created in `self._metrics_by_split` with\n",
    "            the keys 'running', 'count', and 'history'. \n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split to record the metric\n",
    "            metric_name (str): the name of the metric\n",
    "        \"\"\"\n",
    "        self._metrics_by_split[split][metric_name] = {\n",
    "            'running': 0.,\n",
    "            'count': 0,\n",
    "            'history': []\n",
    "        }\n",
    "        \n",
    "    def _update_metric(self, metric_name, metric_value):\n",
    "        \"\"\"Update a metric with an observed value\n",
    "        \n",
    "        Specifically, the running average is updated.\n",
    "        \n",
    "        Args:\n",
    "            metric_name (str): the name of the metric\n",
    "            metric_value (float): the observed value of the metric\n",
    "        \"\"\"\n",
    "        if metric_name not in self._metrics_by_split[self._split]:\n",
    "            self._init_metric(self._split, metric_name)\n",
    "        metric = self._metrics_by_split[self._split][metric_name]\n",
    "        metric['count'] += 1\n",
    "        metric['running'] += (metric_value - metric['running']) / metric['count']\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        \"\"\"Set the dataset split\n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split to set\n",
    "        \"\"\"\n",
    "        self._split = split\n",
    "        \n",
    "    def get_history(self, split, metric_name):\n",
    "        \"\"\"Get the history of values for any metric in any split\n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split\n",
    "            metric_name (str): the target metric\n",
    "            \n",
    "        Returns:\n",
    "            list(float): the running average of each epoch for `metric_name` in `split` \n",
    "        \"\"\"\n",
    "        return self._metrics_by_split[split][metric_name]['history']\n",
    "    \n",
    "    def get_value_of(self, split, metric_name):\n",
    "        \"\"\"Retrieve the running average of any metric in any split\n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split\n",
    "            metric_name (str): the target metric\n",
    "            \n",
    "        Returns:\n",
    "            float: the running average for `metric_name` in `split`\n",
    "        \"\"\"\n",
    "        return self._metrics_by_split[split][metric_name]['running']\n",
    "        \n",
    "    def log_metrics(self, **metrics):\n",
    "        \"\"\"Log some values for some metrics\n",
    "        \n",
    "        Args:\n",
    "            metrics (kwargs): pass keyword args with the form `metric_name=metric_value`\n",
    "                to log the metric values into the attribute `_metrics_by_split`.\n",
    "        \"\"\"\n",
    "        self._split = self.dataset.active_split\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            self._update_metric(metric_name, metric_value)\n",
    "            \n",
    "    def log_epoch_end(self):\n",
    "        \"\"\"Log the end of the epoch. \n",
    "        \n",
    "        Some key functions happen at the end of the epoch:\n",
    "            - for each metric in each split running averages, counts, \n",
    "              and history are updated\n",
    "            - the model is checkpointed if a new best value is observed\n",
    "            - patience is incremented if a new best value is not observed\n",
    "        \"\"\"\n",
    "        for split_dict in self._metrics_by_split.values():\n",
    "            for metric_dict in split_dict.values():\n",
    "                metric_dict['history'].append(metric_dict['running'])\n",
    "                metric_dict['running'] = 0.0\n",
    "                metric_dict['count'] = 0\n",
    "                \n",
    "        if 'loss' in self._metrics_by_split['val']:\n",
    "            val_loss = self._metrics_by_split['val']['loss']['history'][-1]\n",
    "            if val_loss < self._best_loss:\n",
    "                self._best_loss = val_loss\n",
    "                self.save_model()\n",
    "                self.patience = 0\n",
    "            else:\n",
    "                self.patience += 1\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\" Save `model` to `log_dir/model_state_file` \"\"\"\n",
    "        torch.save(self.model.state_dict(), self._full_model_path)\n",
    "    \n",
    "    def reload_best(self):\n",
    "        \"\"\" reload `log_dir/model_state_file` to `model` \"\"\"\n",
    "        if os.path.exists(self._full_model_path):\n",
    "            self.model.load_state_dict(torch.load(self._full_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_amazon_review_dataset(args.dataset_csv, \n",
    "                                     tokenizer_func=simple_word_tokenizer)\n",
    "\n",
    "args.num_embeddings = len(dataset.vectorizer.token_vocab)\n",
    "args.num_classes = len(dataset.vectorizer.label_vocab)\n",
    "\n",
    "# model = ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(args.device)\n",
    "\n",
    "train_state = TrainState(model, 'model.pth', './logs/amazon_reviews/v1')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# progress bars\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='epochs', total=args.num_epochs, position=1)\n",
    "\n",
    "dataset.set_split(\"train\")\n",
    "train_bar = tqdm_notebook(desc='training', total=len(dataset)//args.batch_size)\n",
    "\n",
    "dataset.set_split(\"val\")\n",
    "val_bar = tqdm_notebook(desc='validation', total=len(dataset)//args.batch_size)\n",
    "        \n",
    "\n",
    "try:\n",
    "    for _ in range(args.num_epochs):\n",
    "        model.train()\n",
    "        dataset.set_split(\"train\")\n",
    "        # TODO: deprecate in favor of single source of truth\n",
    "        train_state.set_split(\"train\")\n",
    "        \n",
    "        for batch in generate_batches(dataset, batch_size=args.batch_size, device=args.device):\n",
    "            # Step 1: clear the gradients \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Step 2: compute the outputs\n",
    "            y_prediction = model(batch['x_data'])\n",
    "\n",
    "            # Step 3: compute the loss\n",
    "            loss = loss_func(y_prediction, batch['y_target'])\n",
    "            \n",
    "            # Step 4: propagate the gradients\n",
    "            loss.backward() \n",
    "            \n",
    "            # Step 5: update the model weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Auxillary: logging\n",
    "            train_state.log_metrics(loss=loss.item(), \n",
    "                                    accuracy=compute_accuracy(y_prediction, batch['y_target']))\n",
    "            \n",
    "            train_bar.set_postfix(loss=train_state.get_value_of(split=\"train\", metric_name=\"loss\"),\n",
    "                                  acc=train_state.get_value_of(split=\"train\", metric_name=\"accuracy\"))\n",
    "            train_bar.update()\n",
    "            \n",
    "        # loop over test dataset\n",
    "        \n",
    "        model.eval()\n",
    "        dataset.set_split(\"val\")\n",
    "        train_state.set_split(\"val\")\n",
    "        \n",
    "        for batch in generate_batches(dataset, batch_size=args.batch_size, device=args.device):\n",
    "            # Step 1: compute the outputs\n",
    "            y_prediction = model(batch['x_data'])\n",
    "\n",
    "            # Step 2: compute the loss\n",
    "            loss = loss_func(y_prediction, batch['y_target'])\n",
    "            \n",
    "            # Auxillary: logging\n",
    "            train_state.log_metrics(loss=loss.item(), \n",
    "                                    accuracy=compute_accuracy(y_prediction, batch['y_target']))\n",
    "            \n",
    "            val_bar.set_postfix(loss=train_state.get_value_of(split=\"val\", metric_name=\"loss\"),\n",
    "                                  acc=train_state.get_value_of(split=\"val\", metric_name=\"accuracy\"))\n",
    "            val_bar.update()\n",
    "\n",
    "        \n",
    "        epoch_bar.set_postfix(train_loss=train_state.get_value_of(split=\"train\", \n",
    "                                                                  metric_name=\"loss\"), \n",
    "                              train_accuracy=train_state.get_value_of(split=\"train\", \n",
    "                                                                      metric_name=\"accuracy\"),\n",
    "                              val_loss=train_state.get_value_of(split=\"val\", \n",
    "                                                                metric_name=\"loss\"), \n",
    "                              val_accuracy=train_state.get_value_of(split=\"val\", \n",
    "                                                                    metric_name=\"accuracy\"),\n",
    "                              patience=train_state.patience)\n",
    "        epoch_bar.update()\n",
    "        train_state.log_epoch_end()\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        \n",
    "        if train_state.patience > args.patience_threshold:\n",
    "            break\n",
    "\n",
    "    train_state.reload_best()\n",
    "    model.eval()\n",
    "    dataset.set_split(\"test\")\n",
    "    test_bar = tqdm_notebook(desc='test', total=len(dataset)//args.batch_size)\n",
    "\n",
    "    for batch in generate_batches(dataset, batch_size=args.batch_size, device=args.device):\n",
    "        # Step 1: compute the outputs\n",
    "        y_prediction = model(batch['x_data'])\n",
    "\n",
    "        # Step 2: compute the loss\n",
    "        loss = loss_func(y_prediction, batch['y_target'])\n",
    "\n",
    "        # Auxillary: logging\n",
    "        train_state.log_metrics(loss=loss.item(), \n",
    "                                accuracy=compute_accuracy(y_prediction, batch['y_target']))\n",
    "\n",
    "        test_bar.set_postfix(loss=train_state.get_value_of(split=\"test\", metric_name=\"loss\"),\n",
    "                             acc=train_state.get_value_of(split=\"test\", metric_name=\"accuracy\"))\n",
    "        test_bar.update()            \n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magis",
   "language": "python",
   "name": "magis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
